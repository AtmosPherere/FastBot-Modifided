#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ONNX Runtimeçš„BERTç›¸ä¼¼åº¦è®¡ç®—æµ‹è¯•è„šæœ¬
æ›´æ¥è¿‘C++å®ç°
"""

import sys
import json
import numpy as np
import onnxruntime as ort
import re
import os

class WordPieceTokenizer:
    """æ¨¡æ‹ŸC++ä¸­çš„WordPieceåˆ†è¯å™¨"""
    
    def __init__(self, vocab_path):
        self.vocab = {}
        self.load_vocab(vocab_path)
    
    def load_vocab(self, vocab_path):
        """åŠ è½½è¯æ±‡è¡¨"""
        try:
            with open(vocab_path, 'r', encoding='utf-8') as f:
                for idx, line in enumerate(f):
                    token = line.strip()
                    self.vocab[token] = idx
            print(f"åŠ è½½è¯æ±‡è¡¨æˆåŠŸï¼Œå…± {len(self.vocab)} ä¸ªè¯æ±‡")
        except Exception as e:
            print(f"åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
            # åˆ›å»ºé»˜è®¤è¯æ±‡è¡¨
            self.vocab = {"[UNK]": 0, "[CLS]": 1, "[SEP]": 2, "[PAD]": 3}
    
    def is_chinese(self, c):
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡å­—ç¬¦"""
        return (c >= 0x4E00 and c <= 0x9FFF) or                (c >= 0x3400 and c <= 0x4DBF) or                (c >= 0x20000 and c <= 0x2A6DF)
    
    def clean_token(self, token):
        """æ¸…ç†token"""
        token = token.strip()
        if not any(self.is_chinese(ord(c)) for c in token):
            token = token.lower()
        return token
    
    def basic_tokenize(self, text):
        """åŸºç¡€åˆ†è¯"""
        if not text:
            return []
        
        # åœ¨ä¸­æ–‡å­—ç¬¦å‘¨å›´æ·»åŠ ç©ºæ ¼
        result = []
        i = 0
        while i < len(text):
            if self.is_chinese(ord(text[i])):
                result.append(text[i])
            else:
                # å¤„ç†éä¸­æ–‡å­—ç¬¦
                word = ""
                while i < len(text) and not self.is_chinese(ord(text[i])):
                    if text[i].isspace() or text[i] in ".,!?;:":
                        if word:
                            result.append(word)
                            word = ""
                    else:
                        word += text[i]
                    i += 1
                if word:
                    result.append(word)
                i -= 1
            i += 1
        
        return result
    
    def word_piece_tokenize(self, token):
        """WordPieceåˆ†è¯"""
        if not token:
            return []
        
        token = self.clean_token(token)
        
        if token in self.vocab:
            return [token]
        
        # å°è¯•å­è¯åˆ†å‰²
        subwords = []
        start = 0
        
        while start < len(token):
            end = len(token)
            found = False
            
            while start < end:
                subword = token[start:end]
                if start > 0:
                    subword = "##" + subword
                
                if subword in self.vocab:
                    subwords.append(subword)
                    start = end
                    found = True
                    break
                end -= 1
            
            if not found:
                subwords.append("[UNK]")
                break
        
        return subwords
    
    def tokenize(self, text):
        """å®Œæ•´çš„åˆ†è¯æµç¨‹"""
        if not text:
            return []
        
        tokens = self.basic_tokenize(text)
        word_piece_tokens = []
        
        for token in tokens:
            word_piece_tokens.extend(self.word_piece_tokenize(token))
        
        return word_piece_tokens
    
    def convert_tokens_to_ids(self, tokens):
        """å°†tokensè½¬æ¢ä¸ºIDs"""
        ids = []
        for token in tokens:
            if token in self.vocab:
                ids.append(self.vocab[token])
            else:
                ids.append(self.vocab.get("[UNK]", 0))
        return ids

class ONNXBertSimilarity:
    """ONNX BERTç›¸ä¼¼åº¦è®¡ç®—å™¨"""
    
    def __init__(self, model_path, vocab_path):
        self.session = None
        self.tokenizer = None
        self.load_model(model_path, vocab_path)
    
    def load_model(self, model_path, vocab_path):
        """åŠ è½½ONNXæ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            # åŠ è½½ONNXæ¨¡å‹
            self.session = ort.InferenceSession(model_path)
            print(f"ONNXæ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = WordPieceTokenizer(vocab_path)
            
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            self.session = None
            self.tokenizer = None
    
    def calculate_similarity(self, text1, text2):
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        if not self.session or not self.tokenizer:
            return 0.0
        
        if not text1 or not text2:
            return 0.0
        
        try:
            # åˆ†è¯
            tokens1 = self.tokenizer.tokenize(text1)
            tokens2 = self.tokenizer.tokenize(text2)
            
            # æ·»åŠ ç‰¹æ®Šæ ‡è®°
            tokens1 = ["[CLS]"] + tokens1 + ["[SEP]"]
            tokens2 = ["[CLS]"] + tokens2 + ["[SEP]"]
            
            # è½¬æ¢ä¸ºIDs
            ids1 = self.tokenizer.convert_tokens_to_ids(tokens1)
            ids2 = self.tokenizer.convert_tokens_to_ids(tokens2)
            
            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
            max_length = 128
            ids1 = ids1[:max_length]
            ids2 = ids2[:max_length]
            
            # åˆ›å»ºattention mask
            attention_mask1 = [1] * len(ids1)
            attention_mask2 = [1] * len(ids2)
            
            # å¡«å……åˆ°ç›¸åŒé•¿åº¦
            pad_length = max(len(ids1), len(ids2))
            ids1 += [0] * (pad_length - len(ids1))
            ids2 += [0] * (pad_length - len(ids2))
            attention_mask1 += [0] * (pad_length - len(attention_mask1))
            attention_mask2 += [0] * (pad_length - len(attention_mask2))
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            input_ids1 = np.array([ids1], dtype=np.int64)
            input_ids2 = np.array([ids2], dtype=np.int64)
            attention_mask1 = np.array([attention_mask1], dtype=np.int64)
            attention_mask2 = np.array([attention_mask2], dtype=np.int64)
            
            # è¿è¡Œæ¨¡å‹
            outputs1 = self.session.run(
                ["last_hidden_state", "pooler_output"],
                {
                    "input_ids": input_ids1,
                    "attention_mask": attention_mask1,
                    "token_type_ids": np.zeros_like(input_ids1, dtype=np.int64)
                }
            )
            
            outputs2 = self.session.run(
                ["last_hidden_state", "pooler_output"],
                {
                    "input_ids": input_ids2,
                    "attention_mask": attention_mask2,
                    "token_type_ids": np.zeros_like(input_ids2, dtype=np.int64)
                }
            )
            
            # ä½¿ç”¨pooler_outputè®¡ç®—ç›¸ä¼¼åº¦
            embedding1 = outputs1[1][0]  # pooler_output
            embedding2 = outputs2[1][0]  # pooler_output
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(embedding1, embedding2) / (
                np.linalg.norm(embedding1) * np.linalg.norm(embedding2)
            )
            
            return float(similarity)
            
        except Exception as e:
            print(f"è®¡ç®—ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            return 0.0

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python3 bert_onnx_similarity_test.py <test_cases.json>")
        sys.exit(1)
    
    test_file = sys.argv[1]
    
    # åŠ è½½æµ‹è¯•ç”¨ä¾‹
    with open(test_file, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)
    
    # æ¨¡å‹è·¯å¾„
    model_path = "native/desc/reuse/models/bert-base-multilingual-cased.onnx"
    vocab_path = "native/desc/reuse/models/vocab.txt"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"ONNXæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        sys.exit(1)
    
    if not os.path.exists(vocab_path):
        print(f"è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {vocab_path}")
        sys.exit(1)
    
    # åŠ è½½ONNX BERTæ¨¡å‹
    print("æ­£åœ¨åŠ è½½ONNX BERTæ¨¡å‹...")
    bert_similarity = ONNXBertSimilarity(model_path, vocab_path)
    
    if bert_similarity.session is None:
        print("æ— æ³•åŠ è½½ONNX BERTæ¨¡å‹ï¼Œé€€å‡º")
        sys.exit(1)
    
    print("ONNX BERTæ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print()
    
    # è¿è¡Œæµ‹è¯•
    results = []
    for i, case in enumerate(test_cases, 1):
        print(f"æµ‹è¯• {i}: {case['name']}")
        print(f"  Activity1: {case['activity1']}")
        print(f"  Activity2: {case['activity2']}")
        print(f"  é¢„æœŸç»“æœ: {case['expected']}")
        
        similarity = bert_similarity.calculate_similarity(case['activity1'], case['activity2'])
        
        print(f"  å®é™…ç›¸ä¼¼åº¦: {similarity:.6f}")
        
        # åˆ¤æ–­ç»“æœæ˜¯å¦åˆç†
        if case['name'] == "å®Œå…¨ä¸åŒçš„activity":
            is_reasonable = similarity < 0.5
        elif case['name'] == "ç›¸ä¼¼ä½†ä¸åŒçš„activity":
            is_reasonable = 0.3 < similarity < 0.8
        elif case['name'] == "ç›¸åŒactivity":
            is_reasonable = similarity > 0.9
        elif case['name'] == "ç©ºå­—ç¬¦ä¸²æµ‹è¯•":
            is_reasonable = similarity < 0.3
        elif case['name'] == "ä¸­æ–‡activityåç§°":
            is_reasonable = 0.3 < similarity < 0.8
        elif case['name'] == "è‹±æ–‡vsä¸­æ–‡":
            is_reasonable = 0.3 < similarity < 0.8
        else:
            is_reasonable = True
        
        status = "âœ… åˆç†" if is_reasonable else "âŒ ä¸åˆç†"
        print(f"  ç»“æœè¯„ä¼°: {status}")
        
        results.append({
            'name': case['name'],
            'similarity': similarity,
            'is_reasonable': is_reasonable
        })
        
        print()
    
    # æ€»ç»“
    print("=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    reasonable_count = sum(1 for r in results if r['is_reasonable'])
    total_count = len(results)
    
    print(f"æ€»æµ‹è¯•æ•°: {total_count}")
    print(f"åˆç†ç»“æœæ•°: {reasonable_count}")
    print(f"ä¸åˆç†ç»“æœæ•°: {total_count - reasonable_count}")
    print(f"æˆåŠŸç‡: {reasonable_count/total_count*100:.1f}%")
    
    if reasonable_count == total_count:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼ONNX BERTæ¨¡å‹å·¥ä½œæ­£å¸¸")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        
        # æ˜¾ç¤ºæœ‰é—®é¢˜çš„æµ‹è¯•
        print("
æœ‰é—®é¢˜çš„æµ‹è¯•:")
        for result in results:
            if not result['is_reasonable']:
                print(f"  - {result['name']}: ç›¸ä¼¼åº¦ {result['similarity']:.6f}")

if __name__ == "__main__":
    main()
